{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20943,
     "status": "ok",
     "timestamp": 1677075122737,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "7BBen6pHkY0m",
    "outputId": "66e11ce8-18cd-46e5-9a24-3cb8539f3f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "/content/gdrive/MyDrive/MIT_BIH_ECG\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd \"/content/gdrive/MyDrive/MIT_BIH_ECG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O468p2Qiur_0"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1677075128159,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "l7mDj_Vxf1At"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pywt\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4E_ZFUBvAGZ"
   },
   "source": [
    "### Variables Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1677075134898,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "NaIG3N7yg_Sm",
    "outputId": "af4afc7a-9873-4886-aed0-77d1760105e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "path = './mitbih_database/'\n",
    "output_loc = \"./ECGs_10_classes.hdf5\"\n",
    "\n",
    "classes = ['N', 'L', 'R', 'A', 'V', '/', 'f', 'F', '!', 'j']\n",
    "# classes = ['N', 'L', 'R', 'A', 'V']\n",
    "n_classes = len(classes)        # here is 5\n",
    "count_classes = [0]*n_classes \n",
    "print(count_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM6IGLvfvEdi"
   },
   "source": [
    "### Prepere Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1677075141622,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "OeOA9WXQhFVH"
   },
   "outputs": [],
   "source": [
    "# Read files\n",
    "# The return values of os.walk() is a tupple with three elements.\n",
    "# All filenames in the \"path\" are contained in the third field.\n",
    "# The build-in function is used to return the next item in the iterater.\n",
    "filenames = next(os.walk(path))[2]\n",
    "\n",
    "# Split and save .csv , .txt \n",
    "records = list()\n",
    "annotations = list()\n",
    "filenames.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1677075148080,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "IMC9H8aPhHZ9"
   },
   "outputs": [],
   "source": [
    "# segrefating filenames and annotations\n",
    "for f in filenames:\n",
    "    filename, file_extension = os.path.splitext(f)\n",
    "    \n",
    "    # *.csv\n",
    "    if(file_extension == '.csv'):\n",
    "        records.append(path + filename + file_extension)\n",
    "\n",
    "    # *.txt\n",
    "    else:\n",
    "        annotations.append(path + filename + file_extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL1nGBXLvIJU"
   },
   "source": [
    "### Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66806,
     "status": "ok",
     "timestamp": 1677075217434,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "-6DdA7ikhLFb",
    "outputId": "62817f9b-f2f0-4956-d624-cb0697d30a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109534, 256) (109534,)\n"
     ]
    }
   ],
   "source": [
    "window1_size = 90\n",
    "window2_size = 166\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "# Records\n",
    "for r in range(0, len(records)):\n",
    "    signals = []\n",
    "\n",
    "    with open(records[r], 'rt') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\\\n",
    "        row_index = -1\n",
    "        for row in spamreader:\n",
    "            if(row_index >= 0):\n",
    "                signals.insert(row_index, int(row[1]))\n",
    "            row_index += 1\n",
    "            \n",
    "    # signals = stats.zscore(signals)\n",
    "    \n",
    "    # Read anotations: R position and Arrhythmia class\n",
    "    example_beat_printed = False\n",
    "    with open(annotations[r], 'r') as fileID:\n",
    "        data = fileID.readlines() \n",
    "        beat = list()\n",
    "\n",
    "        for d in range(1, len(data)): # 0 index is Chart Head\n",
    "            splitted = data[d].split(' ')\n",
    "            splitted = filter(None, splitted)\n",
    "            next(splitted) # Time... Clipping\n",
    "            pos = int(next(splitted)) # Sample ID\n",
    "            arrhythmia_type = next(splitted) # Type\n",
    "            if (window1_size <= pos and pos < (len(signals) - window2_size)):\n",
    "                if (arrhythmia_type in classes):\n",
    "                    arrhythmia_index = classes.index(arrhythmia_type)\n",
    "                    count_classes[arrhythmia_index] += 1\n",
    "                    beat = signals[pos-window1_size:pos+window2_size]     ## REPLACE WITH R-PEAK DETECTION\n",
    "                    X.append(beat)\n",
    "                    y.append(arrhythmia_index)\n",
    "\n",
    "# data shape\n",
    "print(np.shape(X), np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "x4J4kNFzdMnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75022 8072 7255 2546 7129 7025 982 802 472 229 "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(y.count(i), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y[:])\n",
    "df[:] = df[:].astype(int)\n",
    "df.to_csv(\"targets.csv\", index=False)\n",
    "np.savetxt(\"./ECG/targets.csv\", df, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5753,
     "status": "ok",
     "timestamp": 1677071696102,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "Mda7OBUfLov0",
    "outputId": "b7d55d1d-fc74-40a4-fa1e-5eda9293d322"
   },
   "outputs": [],
   "source": [
    "pip install pyts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 75921,
     "status": "ok",
     "timestamp": 1677071948963,
     "user": {
      "displayName": "張子姍",
      "userId": "17790380347008705173"
     },
     "user_tz": -480
    },
    "id": "QtN0JuiTPLm0",
    "outputId": "6154a073-b8a2-40fa-f006-7f254d93bd8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109524</th>\n",
       "      <th>109525</th>\n",
       "      <th>109526</th>\n",
       "      <th>109527</th>\n",
       "      <th>109528</th>\n",
       "      <th>109529</th>\n",
       "      <th>109530</th>\n",
       "      <th>109531</th>\n",
       "      <th>109532</th>\n",
       "      <th>109533</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>963</td>\n",
       "      <td>958</td>\n",
       "      <td>949</td>\n",
       "      <td>955</td>\n",
       "      <td>956</td>\n",
       "      <td>958</td>\n",
       "      <td>965</td>\n",
       "      <td>955</td>\n",
       "      <td>953</td>\n",
       "      <td>957</td>\n",
       "      <td>...</td>\n",
       "      <td>970</td>\n",
       "      <td>984</td>\n",
       "      <td>970</td>\n",
       "      <td>968</td>\n",
       "      <td>972</td>\n",
       "      <td>986</td>\n",
       "      <td>977</td>\n",
       "      <td>969</td>\n",
       "      <td>977</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>962</td>\n",
       "      <td>955</td>\n",
       "      <td>947</td>\n",
       "      <td>955</td>\n",
       "      <td>956</td>\n",
       "      <td>958</td>\n",
       "      <td>967</td>\n",
       "      <td>953</td>\n",
       "      <td>956</td>\n",
       "      <td>960</td>\n",
       "      <td>...</td>\n",
       "      <td>973</td>\n",
       "      <td>981</td>\n",
       "      <td>969</td>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "      <td>986</td>\n",
       "      <td>977</td>\n",
       "      <td>970</td>\n",
       "      <td>979</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>964</td>\n",
       "      <td>955</td>\n",
       "      <td>953</td>\n",
       "      <td>954</td>\n",
       "      <td>956</td>\n",
       "      <td>959</td>\n",
       "      <td>968</td>\n",
       "      <td>952</td>\n",
       "      <td>955</td>\n",
       "      <td>959</td>\n",
       "      <td>...</td>\n",
       "      <td>975</td>\n",
       "      <td>980</td>\n",
       "      <td>975</td>\n",
       "      <td>969</td>\n",
       "      <td>970</td>\n",
       "      <td>985</td>\n",
       "      <td>977</td>\n",
       "      <td>970</td>\n",
       "      <td>983</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>963</td>\n",
       "      <td>953</td>\n",
       "      <td>952</td>\n",
       "      <td>953</td>\n",
       "      <td>959</td>\n",
       "      <td>961</td>\n",
       "      <td>969</td>\n",
       "      <td>952</td>\n",
       "      <td>953</td>\n",
       "      <td>961</td>\n",
       "      <td>...</td>\n",
       "      <td>973</td>\n",
       "      <td>978</td>\n",
       "      <td>979</td>\n",
       "      <td>966</td>\n",
       "      <td>971</td>\n",
       "      <td>982</td>\n",
       "      <td>978</td>\n",
       "      <td>971</td>\n",
       "      <td>982</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>966</td>\n",
       "      <td>954</td>\n",
       "      <td>953</td>\n",
       "      <td>955</td>\n",
       "      <td>959</td>\n",
       "      <td>961</td>\n",
       "      <td>968</td>\n",
       "      <td>951</td>\n",
       "      <td>953</td>\n",
       "      <td>960</td>\n",
       "      <td>...</td>\n",
       "      <td>972</td>\n",
       "      <td>980</td>\n",
       "      <td>981</td>\n",
       "      <td>965</td>\n",
       "      <td>970</td>\n",
       "      <td>980</td>\n",
       "      <td>978</td>\n",
       "      <td>971</td>\n",
       "      <td>981</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>959</td>\n",
       "      <td>954</td>\n",
       "      <td>958</td>\n",
       "      <td>957</td>\n",
       "      <td>961</td>\n",
       "      <td>971</td>\n",
       "      <td>951</td>\n",
       "      <td>960</td>\n",
       "      <td>958</td>\n",
       "      <td>962</td>\n",
       "      <td>...</td>\n",
       "      <td>991</td>\n",
       "      <td>975</td>\n",
       "      <td>966</td>\n",
       "      <td>975</td>\n",
       "      <td>987</td>\n",
       "      <td>985</td>\n",
       "      <td>970</td>\n",
       "      <td>978</td>\n",
       "      <td>987</td>\n",
       "      <td>967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>957</td>\n",
       "      <td>956</td>\n",
       "      <td>955</td>\n",
       "      <td>959</td>\n",
       "      <td>961</td>\n",
       "      <td>971</td>\n",
       "      <td>950</td>\n",
       "      <td>960</td>\n",
       "      <td>962</td>\n",
       "      <td>960</td>\n",
       "      <td>...</td>\n",
       "      <td>990</td>\n",
       "      <td>975</td>\n",
       "      <td>968</td>\n",
       "      <td>974</td>\n",
       "      <td>988</td>\n",
       "      <td>983</td>\n",
       "      <td>969</td>\n",
       "      <td>977</td>\n",
       "      <td>985</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>956</td>\n",
       "      <td>953</td>\n",
       "      <td>956</td>\n",
       "      <td>958</td>\n",
       "      <td>959</td>\n",
       "      <td>968</td>\n",
       "      <td>955</td>\n",
       "      <td>957</td>\n",
       "      <td>961</td>\n",
       "      <td>961</td>\n",
       "      <td>...</td>\n",
       "      <td>989</td>\n",
       "      <td>975</td>\n",
       "      <td>970</td>\n",
       "      <td>971</td>\n",
       "      <td>987</td>\n",
       "      <td>981</td>\n",
       "      <td>970</td>\n",
       "      <td>979</td>\n",
       "      <td>987</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>957</td>\n",
       "      <td>954</td>\n",
       "      <td>957</td>\n",
       "      <td>961</td>\n",
       "      <td>960</td>\n",
       "      <td>971</td>\n",
       "      <td>956</td>\n",
       "      <td>957</td>\n",
       "      <td>959</td>\n",
       "      <td>961</td>\n",
       "      <td>...</td>\n",
       "      <td>984</td>\n",
       "      <td>972</td>\n",
       "      <td>971</td>\n",
       "      <td>970</td>\n",
       "      <td>986</td>\n",
       "      <td>979</td>\n",
       "      <td>970</td>\n",
       "      <td>976</td>\n",
       "      <td>986</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>957</td>\n",
       "      <td>951</td>\n",
       "      <td>958</td>\n",
       "      <td>957</td>\n",
       "      <td>962</td>\n",
       "      <td>975</td>\n",
       "      <td>956</td>\n",
       "      <td>959</td>\n",
       "      <td>959</td>\n",
       "      <td>964</td>\n",
       "      <td>...</td>\n",
       "      <td>983</td>\n",
       "      <td>971</td>\n",
       "      <td>970</td>\n",
       "      <td>973</td>\n",
       "      <td>986</td>\n",
       "      <td>979</td>\n",
       "      <td>971</td>\n",
       "      <td>978</td>\n",
       "      <td>988</td>\n",
       "      <td>962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 109534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1       2       3       4       5       6       7       8       \\\n",
       "0       963     958     949     955     956     958     965     955     953   \n",
       "1       962     955     947     955     956     958     967     953     956   \n",
       "2       964     955     953     954     956     959     968     952     955   \n",
       "3       963     953     952     953     959     961     969     952     953   \n",
       "4       966     954     953     955     959     961     968     951     953   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "251     959     954     958     957     961     971     951     960     958   \n",
       "252     957     956     955     959     961     971     950     960     962   \n",
       "253     956     953     956     958     959     968     955     957     961   \n",
       "254     957     954     957     961     960     971     956     957     959   \n",
       "255     957     951     958     957     962     975     956     959     959   \n",
       "\n",
       "     9       ...  109524  109525  109526  109527  109528  109529  109530  \\\n",
       "0       957  ...     970     984     970     968     972     986     977   \n",
       "1       960  ...     973     981     969     971     971     986     977   \n",
       "2       959  ...     975     980     975     969     970     985     977   \n",
       "3       961  ...     973     978     979     966     971     982     978   \n",
       "4       960  ...     972     980     981     965     970     980     978   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "251     962  ...     991     975     966     975     987     985     970   \n",
       "252     960  ...     990     975     968     974     988     983     969   \n",
       "253     961  ...     989     975     970     971     987     981     970   \n",
       "254     961  ...     984     972     971     970     986     979     970   \n",
       "255     964  ...     983     971     970     973     986     979     971   \n",
       "\n",
       "     109531  109532  109533  \n",
       "0       969     977     987  \n",
       "1       970     979     986  \n",
       "2       970     983     987  \n",
       "3       971     982     988  \n",
       "4       971     981     988  \n",
       "..      ...     ...     ...  \n",
       "251     978     987     967  \n",
       "252     977     985     966  \n",
       "253     979     987     962  \n",
       "254     976     986     961  \n",
       "255     978     988     962  \n",
       "\n",
       "[256 rows x 109534 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X[:])\n",
    "df = df.T\n",
    "df.to_csv(\"ECG_signals.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3b2es1hNjrg"
   },
   "outputs": [],
   "source": [
    "# Author: strongnine (WeChat: strongnine)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "#sin_data = np.loadtxt('./ECG_signals.csv', delimiter=\",\", skiprows=0).reshape(1, -1)  # 加载数据 (load the source data)\n",
    "f1 = \"ECG_signals.csv\"\n",
    "file = pd.read_csv(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1fhq82AsAP6LChHHcEinObwPWpJkcaz7j"
    },
    "id": "qI43_sBPMC0n",
    "outputId": "0d3c04e2-e22f-4460-db6b-6baf03dfb544"
   },
   "outputs": [],
   "source": [
    "for i in range(2016, len(file.columns)):\n",
    "    sin_data = (file.iloc[:, i].to_numpy()).reshape(1, -1)\n",
    "    image_size = 256  # 生成的 GAF 图片的大小 (the size of each GAF image)\n",
    "\n",
    "    # `method` 的可选参数有：`summation` and `difference`\n",
    "    # The optional parameters of argument `method`: `summation` and `difference`\n",
    "    gasf = GramianAngularField(image_size=image_size, method='summation')\n",
    "    sin_gasf = gasf.fit_transform(sin_data)\n",
    "\n",
    "    gadf = GramianAngularField(image_size=image_size, method='difference')\n",
    "    sin_gadf = gadf.fit_transform(sin_data)\n",
    "    imges = [sin_gasf[0], sin_gadf[0]]\n",
    "    titles = ['Summation', 'Difference']\n",
    "\n",
    "    # 两种方法的可视化差异对比\n",
    "    # Comparison of two different methods\n",
    "    #fig, axs = plt.subplots(1, 2, constrained_layout=True)\n",
    "    #for img, title, ax in zip(imges, titles, axs):\n",
    "        #ax.imshow(img)\n",
    "        #ax.set_title(title)\n",
    "    #fig.suptitle('GramianAngularField {}'.format(i+1), y=0.94, fontsize=16)\n",
    "    #plt.margins(0, 0)\n",
    "    #plt.savefig(\"./PDF/GramianAngularField{}.pdf\".format(i+1), pad_inches=0)\n",
    "    #plt.show()\n",
    "    if i==9:\n",
    "        print(i+1)\n",
    "    else:\n",
    "        print(i+1, end=\" \")\n",
    "\n",
    "    #image.imsave(\"./ECG/Summation/images{}/GAF_of_Sin{}.png\".format(y[i]+1, i+1), sin_gasf[0])  # 保存图片 (save image)\n",
    "    image.imsave(\"./ECG/Difference/images{}/GAF_of_Sin{}.png\".format(y[i]+1, i+1), sin_gadf[0])  # 保存图片 (save image)\n",
    "    #np.savetxt(\"./ECG/CSV/Summation/GAF_of_Sin{}.csv\".format(i+1), sin_gasf[0], delimiter=',')  # 保存数据为 csv 文件\n",
    "    #np.savetxt(\"./ECG/CSV/Difference/GAF_of_Sin{}.csv\".format(i+1), sin_gadf[0], delimiter=',')  # 保存数据为 csv 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "data1 = []\n",
    "#class_name=[]\n",
    "\n",
    "for i in range(10):\n",
    "    img_folder = f'./ECG/Difference/images{i+1}'\n",
    "    img_data_array=[]\n",
    "    #for dir1 in os.listdir(img_folder):\n",
    "    for file in os.listdir(os.path.join(img_folder)):\n",
    "\n",
    "        image_path= os.path.join(img_folder, file)\n",
    "        image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "        image=np.array(image)\n",
    "        image = image.astype('float16')\n",
    "        image /= 255 \n",
    "        img_data_array.append(image)\n",
    "        #class_name.append(file)\n",
    "    data1.append(img_data_array)\n",
    "    print(i)\n",
    "print(len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87623 87623\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trains1 = []\n",
    "X_tests1 = []\n",
    "y_trains1 = []\n",
    "y_tests1 = []\n",
    "label = []\n",
    "for i in range(10):\n",
    "    temp = [i for j in range(y.count(i))]\n",
    "    label.append(temp)\n",
    "temp = list(data1[0])\n",
    "X_trains, X_tests, y_trains, y_tests = train_test_split(temp, label[0], test_size=0.2, random_state=1)\n",
    "       \n",
    "    \n",
    "for i in range(1, 10):\n",
    "    #data = os.listdir(f'./ECG/Difference/images{i+1}')\n",
    "    #print(len(data), len(label[i]))\n",
    "    temp = list(data[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp, label[i], test_size=0.2, random_state=1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(img_data_array, y, test_size=0.2, random_state=1)\n",
    "   \n",
    "    # Merge inputs and targets\n",
    "    X_trains1 = np.concatenate((X_trains1, X_train), axis=0)\n",
    "    X_tests1 = np.concatenate((X_tests1, X_test), axis=0)\n",
    "    y_trains1 = np.concatenate((y_trains1, y_train), axis=0)\n",
    "    y_tests1 = np.concatenate((y_tests1, y_test), axis=0)\n",
    "\n",
    "print(len(X_trains1), len(y_trains1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               590080    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643,658\n",
      "Trainable params: 643,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 32\n",
    "input_shape = (32, 32, 3)\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 10\n",
    "no_epochs = 10\n",
    "optimizer = Adam()\n",
    "verbosity = 1\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Merge inputs and targets\n",
    "#inputs = np.concatenate((input_train, input_test), axis=0)\n",
    "#targets = np.concatenate((target_train, target_test), axis=0)\n",
    "inputs1 = np.concatenate((X_trains1, X_tests1), axis=0)\n",
    "targets1 = np.concatenate((y_trains1, y_tests1), axis=0)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dense(no_classes, activation='softmax'))\n",
    "print(model1.summary())\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "#model.save('./ECG/model/my_model.h5')\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 53s 17ms/step - loss: 0.1842 - accuracy: 0.9506\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0759 - accuracy: 0.9794\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 81s 26ms/step - loss: 0.0577 - accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0486 - accuracy: 0.9860\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0420 - accuracy: 0.9880\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0373 - accuracy: 0.9894\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0325 - accuracy: 0.9901\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0307 - accuracy: 0.9910\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0282 - accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0259 - accuracy: 0.9921\n",
      "Score for fold 1: loss of 0.05199152231216431; accuracy of 98.62150549888611%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0280 - accuracy: 0.9920\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 82s 27ms/step - loss: 0.0241 - accuracy: 0.9928\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0224 - accuracy: 0.9933\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 71s 23ms/step - loss: 0.0213 - accuracy: 0.9934\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0187 - accuracy: 0.9941\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0165 - accuracy: 0.9951\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0171 - accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0162 - accuracy: 0.9951\n",
      "Score for fold 2: loss of 0.031159762293100357; accuracy of 99.2422878742218%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0189 - accuracy: 0.9944\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0153 - accuracy: 0.9954\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0157 - accuracy: 0.9950\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0151 - accuracy: 0.9953\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0146 - accuracy: 0.9954\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0136 - accuracy: 0.9960\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0146 - accuracy: 0.9957\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0147 - accuracy: 0.9954\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 69s 23ms/step - loss: 0.0134 - accuracy: 0.9960\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0131 - accuracy: 0.9960\n",
      "Score for fold 3: loss of 0.020790966227650642; accuracy of 99.40661191940308%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0157 - accuracy: 0.9953\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0138 - accuracy: 0.9961\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 66s 22ms/step - loss: 0.0138 - accuracy: 0.9958\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0116 - accuracy: 0.9962\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0121 - accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0121 - accuracy: 0.9962\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0098 - accuracy: 0.9971\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0118 - accuracy: 0.9964\n",
      "Score for fold 4: loss of 0.018943728879094124; accuracy of 99.37922358512878%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0126 - accuracy: 0.9963\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0129 - accuracy: 0.9965\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0120 - accuracy: 0.9964\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0114 - accuracy: 0.9968\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0098 - accuracy: 0.9970\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0119 - accuracy: 0.9964\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0104 - accuracy: 0.9968\n",
      "Score for fold 5: loss of 0.028834830969572067; accuracy of 99.3882954120636%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0133 - accuracy: 0.9964\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0109 - accuracy: 0.9969\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0110 - accuracy: 0.9967\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0112 - accuracy: 0.9968\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0109 - accuracy: 0.9971\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 70s 23ms/step - loss: 0.0137 - accuracy: 0.9965\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 71s 23ms/step - loss: 0.0095 - accuracy: 0.9973\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0099 - accuracy: 0.9971\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0115 - accuracy: 0.9969\n",
      "Score for fold 6: loss of 0.01931675896048546; accuracy of 99.47959184646606%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0112 - accuracy: 0.9969\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0109 - accuracy: 0.9971\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0084 - accuracy: 0.9974\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0115 - accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Score for fold 7: loss of 0.010710549540817738; accuracy of 99.72610473632812%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0115 - accuracy: 0.9969\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0094 - accuracy: 0.9973\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0111 - accuracy: 0.9970\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0105 - accuracy: 0.9972\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0105 - accuracy: 0.9973\n",
      "Score for fold 8: loss of 0.00841793604195118; accuracy of 99.77174997329712%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0100 - accuracy: 0.9971\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 66s 22ms/step - loss: 0.0117 - accuracy: 0.9968\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 80s 26ms/step - loss: 0.0126 - accuracy: 0.9970\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0116 - accuracy: 0.9972\n",
      "Score for fold 9: loss of 0.010919823311269283; accuracy of 99.75349307060242%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0096 - accuracy: 0.9973\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0112 - accuracy: 0.9971\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 70s 23ms/step - loss: 0.0107 - accuracy: 0.9972\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0110 - accuracy: 0.9973\n",
      "Score for fold 10: loss of 0.021101614460349083; accuracy of 99.60741400718689%\n"
     ]
    }
   ],
   "source": [
    "#re_model = keras.models.load_model(\"./ECG/model/my_model.h5\")\n",
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs1, targets1):\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history1 = model1.fit(inputs1[train], targets1[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity,\n",
    "              callbacks=[callback])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores1 = model1.evaluate(inputs1[test], targets1[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model1.metrics_names[0]} of {scores1[0]}; {model1.metrics_names[1]} of {scores1[1]*100}%')\n",
    "    acc_per_fold.append(scores1[1] * 100)\n",
    "    loss_per_fold.append(scores1[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "data = []\n",
    "#class_name=[]\n",
    "\n",
    "for i in range(10):\n",
    "    img_folder = f'./ECG/Summation/images{i+1}'\n",
    "    img_data_array=[]\n",
    "    #for dir1 in os.listdir(img_folder):\n",
    "    for file in os.listdir(os.path.join(img_folder)):\n",
    "\n",
    "        image_path= os.path.join(img_folder, file)\n",
    "        image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
    "        image=np.array(image)\n",
    "        image = image.astype('float16')\n",
    "        image /= 255 \n",
    "        img_data_array.append(image)\n",
    "        #class_name.append(file)\n",
    "    data.append(img_data_array)\n",
    "    print(i)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87623 87623\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trains = []\n",
    "X_tests = []\n",
    "y_trains = []\n",
    "y_tests = []\n",
    "label = []\n",
    "for i in range(10):\n",
    "    temp = [i for j in range(y.count(i))]\n",
    "    label.append(temp)\n",
    "temp = list(data[0])\n",
    "X_trains, X_tests, y_trains, y_tests = train_test_split(temp, label[0], test_size=0.2, random_state=1)\n",
    "       \n",
    "    \n",
    "for i in range(1, 10):\n",
    "    #data = os.listdir(f'./ECG/Difference/images{i+1}')\n",
    "    #print(len(data), len(label[i]))\n",
    "    temp = list(data[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(temp, label[i], test_size=0.2, random_state=1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(img_data_array, y, test_size=0.2, random_state=1)\n",
    "   \n",
    "    # Merge inputs and targets\n",
    "    X_trains = np.concatenate((X_trains, X_train), axis=0)\n",
    "    X_tests = np.concatenate((X_tests, X_test), axis=0)\n",
    "    y_trains = np.concatenate((y_trains, y_train), axis=0)\n",
    "    y_tests = np.concatenate((y_tests, y_test), axis=0)\n",
    "\n",
    "print(len(X_trains), len(y_trains))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 15, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 256)               590080    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643,658\n",
      "Trainable params: 643,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 32\n",
    "input_shape = (32, 32, 3)\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 10\n",
    "no_epochs = 10\n",
    "optimizer = Adam()\n",
    "verbosity = 1\n",
    "num_folds = 10\n",
    "\n",
    "\n",
    "# Define per-fold score containers\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Merge inputs and targets\n",
    "#inputs = np.concatenate((input_train, input_test), axis=0)\n",
    "#targets = np.concatenate((target_train, target_test), axis=0)\n",
    "inputs = np.concatenate((X_trains, X_tests), axis=0)\n",
    "targets = np.concatenate((y_trains, y_tests), axis=0)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "#model.save('./ECG/model/my_model.h5')\n",
    "print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 78s 25ms/step - loss: 0.2068 - accuracy: 0.9459\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 143s 46ms/step - loss: 0.0873 - accuracy: 0.9762\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0678 - accuracy: 0.9810\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0562 - accuracy: 0.9838\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0484 - accuracy: 0.9862\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0426 - accuracy: 0.9876\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0384 - accuracy: 0.9884\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 102s 33ms/step - loss: 0.0340 - accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0304 - accuracy: 0.9909\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0294 - accuracy: 0.9911\n",
      "Score for fold 1: loss of 0.07727085053920746; accuracy of 98.33850860595703%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 98s 32ms/step - loss: 0.0318 - accuracy: 0.9907\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0272 - accuracy: 0.9917\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 105s 34ms/step - loss: 0.0253 - accuracy: 0.9923\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0245 - accuracy: 0.9924\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 103s 34ms/step - loss: 0.0212 - accuracy: 0.9938\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 105s 34ms/step - loss: 0.0219 - accuracy: 0.9932\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0201 - accuracy: 0.9936\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0193 - accuracy: 0.9939\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 120s 39ms/step - loss: 0.0178 - accuracy: 0.9945\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Score for fold 2: loss of 0.04149759188294411; accuracy of 99.05057549476624%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0207 - accuracy: 0.9940\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 103s 34ms/step - loss: 0.0183 - accuracy: 0.9945\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 102s 33ms/step - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0169 - accuracy: 0.9949\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0155 - accuracy: 0.9953\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0161 - accuracy: 0.9953\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0154 - accuracy: 0.9953\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0151 - accuracy: 0.9954\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0140 - accuracy: 0.9957\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0151 - accuracy: 0.9956\n",
      "Score for fold 3: loss of 0.037436820566654205; accuracy of 99.08708930015564%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0164 - accuracy: 0.9950\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0135 - accuracy: 0.9961\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0147 - accuracy: 0.9954\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 109s 35ms/step - loss: 0.0127 - accuracy: 0.9961\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0135 - accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0133 - accuracy: 0.9962\n",
      "Score for fold 4: loss of 0.021438004449009895; accuracy of 99.3883490562439%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0140 - accuracy: 0.9960\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0134 - accuracy: 0.9960\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0122 - accuracy: 0.9962\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 125s 40ms/step - loss: 0.0133 - accuracy: 0.9961\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0123 - accuracy: 0.9962\n",
      "Score for fold 5: loss of 0.023886283859610558; accuracy of 99.3609070777893%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0132 - accuracy: 0.9960\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 111s 36ms/step - loss: 0.0130 - accuracy: 0.9962\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 109s 35ms/step - loss: 0.0115 - accuracy: 0.9965\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 118s 38ms/step - loss: 0.0131 - accuracy: 0.9963\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0115 - accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 124s 40ms/step - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 141s 46ms/step - loss: 0.0135 - accuracy: 0.9962\n",
      "Epoch 8/10\n",
      "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 9/10\n",
      "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0120 - accuracy: 0.9963\n",
      "Epoch 10/10\n",
      "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0108 - accuracy: 0.9967\n",
      "Score for fold 6: loss of 0.01732526533305645; accuracy of 99.55263137817383%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0133 - accuracy: 0.9966\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0116 - accuracy: 0.9966\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0115 - accuracy: 0.9966\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0112 - accuracy: 0.9968\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0122 - accuracy: 0.9966\n",
      "Score for fold 7: loss of 0.011116073466837406; accuracy of 99.67132210731506%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0097 - accuracy: 0.9973\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0123 - accuracy: 0.9966\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0116 - accuracy: 0.9968\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0114 - accuracy: 0.9967\n",
      "Score for fold 8: loss of 0.014107241295278072; accuracy of 99.65306520462036%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 118s 38ms/step - loss: 0.0120 - accuracy: 0.9967\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0114 - accuracy: 0.9969\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0111 - accuracy: 0.9969\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 120s 39ms/step - loss: 0.0102 - accuracy: 0.9972\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0102 - accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0103 - accuracy: 0.9971\n",
      "Epoch 7/10\n",
      "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0105 - accuracy: 0.9970\n",
      "Score for fold 9: loss of 0.016758309677243233; accuracy of 99.52524304389954%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "3081/3081 [==============================] - 114s 37ms/step - loss: 0.0142 - accuracy: 0.9965\n",
      "Epoch 2/10\n",
      "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0108 - accuracy: 0.9970\n",
      "Epoch 3/10\n",
      "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0115 - accuracy: 0.9969\n",
      "Epoch 4/10\n",
      "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 5/10\n",
      "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Score for fold 10: loss of 0.007950237020850182; accuracy of 99.7900128364563%\n"
     ]
    }
   ],
   "source": [
    "#re_model = keras.models.load_model(\"./ECG/model/my_model.h5\")\n",
    "import tensorflow as tf\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=batch_size,\n",
    "              epochs=no_epochs,\n",
    "              verbose=verbosity,\n",
    "              callbacks=[callback])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/685 [==============================] - 5s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9523    1.0000    0.9756     15005\n",
      "           1     1.0000    0.9882    0.9941      1615\n",
      "           2     1.0000    0.8415    0.9139      1451\n",
      "           3     1.0000    0.6961    0.8208       510\n",
      "           4     1.0000    0.8962    0.9453      1426\n",
      "           5     1.0000    0.9907    0.9954      1405\n",
      "           6     1.0000    0.8122    0.8964       197\n",
      "           7     1.0000    0.2174    0.3571       161\n",
      "           8     1.0000    1.0000    1.0000        95\n",
      "           9     1.0000    0.5000    0.6667        46\n",
      "\n",
      "    accuracy                         0.9657     21911\n",
      "   macro avg     0.9952    0.7942    0.8565     21911\n",
      "weighted avg     0.9674    0.9657    0.9628     21911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = model.predict(X_tests)\n",
    "preds = np.argmax(preds.astype('int'), axis=1)\n",
    "y_tests = y_tests.flatten()\n",
    "print(classification_report(y_tests, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x214003e3b20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "#plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.99, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
