{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BBen6pHkY0m",
        "outputId": "66e11ce8-18cd-46e5-9a24-3cb8539f3f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/MIT_BIH_ECG\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd \"/content/gdrive/MyDrive/MIT_BIH_ECG\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O468p2Qiur_0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7mDj_Vxf1At"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import pywt\n",
        "from scipy import stats\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4E_ZFUBvAGZ"
      },
      "source": [
        "### Variables Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaIG3N7yg_Sm",
        "outputId": "af4afc7a-9873-4886-aed0-77d1760105e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "path = './mitbih_database/'\n",
        "output_loc = \"./ECGs_10_classes.hdf5\"\n",
        "\n",
        "classes = ['N', 'L', 'R', 'A', 'V', '/', 'f', 'F', '!', 'j']\n",
        "# classes = ['N', 'L', 'R', 'A', 'V']\n",
        "n_classes = len(classes)        # here is 5\n",
        "count_classes = [0]*n_classes\n",
        "print(count_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM6IGLvfvEdi"
      },
      "source": [
        "### Prepere Input Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeOA9WXQhFVH"
      },
      "outputs": [],
      "source": [
        "# Read files\n",
        "# The return values of os.walk() is a tupple with three elements.\n",
        "# All filenames in the \"path\" are contained in the third field.\n",
        "# The build-in function is used to return the next item in the iterater.\n",
        "filenames = next(os.walk(path))[2]\n",
        "\n",
        "# Split and save .csv , .txt\n",
        "records = list()\n",
        "annotations = list()\n",
        "filenames.sort()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMC9H8aPhHZ9"
      },
      "outputs": [],
      "source": [
        "# segrefating filenames and annotations\n",
        "for f in filenames:\n",
        "    filename, file_extension = os.path.splitext(f)\n",
        "\n",
        "    # *.csv\n",
        "    if(file_extension == '.csv'):\n",
        "        records.append(path + filename + file_extension)\n",
        "\n",
        "    # *.txt\n",
        "    else:\n",
        "        annotations.append(path + filename + file_extension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL1nGBXLvIJU"
      },
      "source": [
        "### Data Extraction and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6DdA7ikhLFb",
        "outputId": "62817f9b-f2f0-4956-d624-cb0697d30a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(109534, 256) (109534,)\n"
          ]
        }
      ],
      "source": [
        "window1_size = 90\n",
        "window2_size = 166\n",
        "X = list()\n",
        "y = list()\n",
        "\n",
        "# Records\n",
        "for r in range(0, len(records)):\n",
        "    signals = []\n",
        "\n",
        "    with open(records[r], 'rt') as csvfile:\n",
        "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|') # read CSV file\\\n",
        "        row_index = -1\n",
        "        for row in spamreader:\n",
        "            if(row_index >= 0):\n",
        "                signals.insert(row_index, int(row[1]))\n",
        "            row_index += 1\n",
        "\n",
        "    # signals = stats.zscore(signals)\n",
        "\n",
        "    # Read anotations: R position and Arrhythmia class\n",
        "    example_beat_printed = False\n",
        "    with open(annotations[r], 'r') as fileID:\n",
        "        data = fileID.readlines()\n",
        "        beat = list()\n",
        "\n",
        "        for d in range(1, len(data)): # 0 index is Chart Head\n",
        "            splitted = data[d].split(' ')\n",
        "            splitted = filter(None, splitted)\n",
        "            next(splitted) # Time... Clipping\n",
        "            pos = int(next(splitted)) # Sample ID\n",
        "            arrhythmia_type = next(splitted) # Type\n",
        "            if (window1_size <= pos and pos < (len(signals) - window2_size)):\n",
        "                if (arrhythmia_type in classes):\n",
        "                    arrhythmia_index = classes.index(arrhythmia_type)\n",
        "                    count_classes[arrhythmia_index] += 1\n",
        "                    beat = signals[pos-window1_size:pos+window2_size]     ## REPLACE WITH R-PEAK DETECTION\n",
        "                    X.append(beat)\n",
        "                    y.append(arrhythmia_index)\n",
        "\n",
        "# data shape\n",
        "print(np.shape(X), np.shape(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSDf0JbrqVTq"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(y[:])\n",
        "df[:] = df[:].astype(int)\n",
        "df.to_csv(\"targets.csv\", index=False)\n",
        "np.savetxt(\"./ECG/targets.csv\", df, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mda7OBUfLov0"
      },
      "outputs": [],
      "source": [
        "pip install pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QtN0JuiTPLm0",
        "outputId": "6154a073-b8a2-40fa-f006-7f254d93bd8a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>109524</th>\n",
              "      <th>109525</th>\n",
              "      <th>109526</th>\n",
              "      <th>109527</th>\n",
              "      <th>109528</th>\n",
              "      <th>109529</th>\n",
              "      <th>109530</th>\n",
              "      <th>109531</th>\n",
              "      <th>109532</th>\n",
              "      <th>109533</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>963</td>\n",
              "      <td>958</td>\n",
              "      <td>949</td>\n",
              "      <td>955</td>\n",
              "      <td>956</td>\n",
              "      <td>958</td>\n",
              "      <td>965</td>\n",
              "      <td>955</td>\n",
              "      <td>953</td>\n",
              "      <td>957</td>\n",
              "      <td>...</td>\n",
              "      <td>970</td>\n",
              "      <td>984</td>\n",
              "      <td>970</td>\n",
              "      <td>968</td>\n",
              "      <td>972</td>\n",
              "      <td>986</td>\n",
              "      <td>977</td>\n",
              "      <td>969</td>\n",
              "      <td>977</td>\n",
              "      <td>987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>962</td>\n",
              "      <td>955</td>\n",
              "      <td>947</td>\n",
              "      <td>955</td>\n",
              "      <td>956</td>\n",
              "      <td>958</td>\n",
              "      <td>967</td>\n",
              "      <td>953</td>\n",
              "      <td>956</td>\n",
              "      <td>960</td>\n",
              "      <td>...</td>\n",
              "      <td>973</td>\n",
              "      <td>981</td>\n",
              "      <td>969</td>\n",
              "      <td>971</td>\n",
              "      <td>971</td>\n",
              "      <td>986</td>\n",
              "      <td>977</td>\n",
              "      <td>970</td>\n",
              "      <td>979</td>\n",
              "      <td>986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>964</td>\n",
              "      <td>955</td>\n",
              "      <td>953</td>\n",
              "      <td>954</td>\n",
              "      <td>956</td>\n",
              "      <td>959</td>\n",
              "      <td>968</td>\n",
              "      <td>952</td>\n",
              "      <td>955</td>\n",
              "      <td>959</td>\n",
              "      <td>...</td>\n",
              "      <td>975</td>\n",
              "      <td>980</td>\n",
              "      <td>975</td>\n",
              "      <td>969</td>\n",
              "      <td>970</td>\n",
              "      <td>985</td>\n",
              "      <td>977</td>\n",
              "      <td>970</td>\n",
              "      <td>983</td>\n",
              "      <td>987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>963</td>\n",
              "      <td>953</td>\n",
              "      <td>952</td>\n",
              "      <td>953</td>\n",
              "      <td>959</td>\n",
              "      <td>961</td>\n",
              "      <td>969</td>\n",
              "      <td>952</td>\n",
              "      <td>953</td>\n",
              "      <td>961</td>\n",
              "      <td>...</td>\n",
              "      <td>973</td>\n",
              "      <td>978</td>\n",
              "      <td>979</td>\n",
              "      <td>966</td>\n",
              "      <td>971</td>\n",
              "      <td>982</td>\n",
              "      <td>978</td>\n",
              "      <td>971</td>\n",
              "      <td>982</td>\n",
              "      <td>988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>966</td>\n",
              "      <td>954</td>\n",
              "      <td>953</td>\n",
              "      <td>955</td>\n",
              "      <td>959</td>\n",
              "      <td>961</td>\n",
              "      <td>968</td>\n",
              "      <td>951</td>\n",
              "      <td>953</td>\n",
              "      <td>960</td>\n",
              "      <td>...</td>\n",
              "      <td>972</td>\n",
              "      <td>980</td>\n",
              "      <td>981</td>\n",
              "      <td>965</td>\n",
              "      <td>970</td>\n",
              "      <td>980</td>\n",
              "      <td>978</td>\n",
              "      <td>971</td>\n",
              "      <td>981</td>\n",
              "      <td>988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>959</td>\n",
              "      <td>954</td>\n",
              "      <td>958</td>\n",
              "      <td>957</td>\n",
              "      <td>961</td>\n",
              "      <td>971</td>\n",
              "      <td>951</td>\n",
              "      <td>960</td>\n",
              "      <td>958</td>\n",
              "      <td>962</td>\n",
              "      <td>...</td>\n",
              "      <td>991</td>\n",
              "      <td>975</td>\n",
              "      <td>966</td>\n",
              "      <td>975</td>\n",
              "      <td>987</td>\n",
              "      <td>985</td>\n",
              "      <td>970</td>\n",
              "      <td>978</td>\n",
              "      <td>987</td>\n",
              "      <td>967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>957</td>\n",
              "      <td>956</td>\n",
              "      <td>955</td>\n",
              "      <td>959</td>\n",
              "      <td>961</td>\n",
              "      <td>971</td>\n",
              "      <td>950</td>\n",
              "      <td>960</td>\n",
              "      <td>962</td>\n",
              "      <td>960</td>\n",
              "      <td>...</td>\n",
              "      <td>990</td>\n",
              "      <td>975</td>\n",
              "      <td>968</td>\n",
              "      <td>974</td>\n",
              "      <td>988</td>\n",
              "      <td>983</td>\n",
              "      <td>969</td>\n",
              "      <td>977</td>\n",
              "      <td>985</td>\n",
              "      <td>966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>956</td>\n",
              "      <td>953</td>\n",
              "      <td>956</td>\n",
              "      <td>958</td>\n",
              "      <td>959</td>\n",
              "      <td>968</td>\n",
              "      <td>955</td>\n",
              "      <td>957</td>\n",
              "      <td>961</td>\n",
              "      <td>961</td>\n",
              "      <td>...</td>\n",
              "      <td>989</td>\n",
              "      <td>975</td>\n",
              "      <td>970</td>\n",
              "      <td>971</td>\n",
              "      <td>987</td>\n",
              "      <td>981</td>\n",
              "      <td>970</td>\n",
              "      <td>979</td>\n",
              "      <td>987</td>\n",
              "      <td>962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>957</td>\n",
              "      <td>954</td>\n",
              "      <td>957</td>\n",
              "      <td>961</td>\n",
              "      <td>960</td>\n",
              "      <td>971</td>\n",
              "      <td>956</td>\n",
              "      <td>957</td>\n",
              "      <td>959</td>\n",
              "      <td>961</td>\n",
              "      <td>...</td>\n",
              "      <td>984</td>\n",
              "      <td>972</td>\n",
              "      <td>971</td>\n",
              "      <td>970</td>\n",
              "      <td>986</td>\n",
              "      <td>979</td>\n",
              "      <td>970</td>\n",
              "      <td>976</td>\n",
              "      <td>986</td>\n",
              "      <td>961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>957</td>\n",
              "      <td>951</td>\n",
              "      <td>958</td>\n",
              "      <td>957</td>\n",
              "      <td>962</td>\n",
              "      <td>975</td>\n",
              "      <td>956</td>\n",
              "      <td>959</td>\n",
              "      <td>959</td>\n",
              "      <td>964</td>\n",
              "      <td>...</td>\n",
              "      <td>983</td>\n",
              "      <td>971</td>\n",
              "      <td>970</td>\n",
              "      <td>973</td>\n",
              "      <td>986</td>\n",
              "      <td>979</td>\n",
              "      <td>971</td>\n",
              "      <td>978</td>\n",
              "      <td>988</td>\n",
              "      <td>962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows Ã— 109534 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0       1       2       3       4       5       6       7       8       \\\n",
              "0       963     958     949     955     956     958     965     955     953   \n",
              "1       962     955     947     955     956     958     967     953     956   \n",
              "2       964     955     953     954     956     959     968     952     955   \n",
              "3       963     953     952     953     959     961     969     952     953   \n",
              "4       966     954     953     955     959     961     968     951     953   \n",
              "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "251     959     954     958     957     961     971     951     960     958   \n",
              "252     957     956     955     959     961     971     950     960     962   \n",
              "253     956     953     956     958     959     968     955     957     961   \n",
              "254     957     954     957     961     960     971     956     957     959   \n",
              "255     957     951     958     957     962     975     956     959     959   \n",
              "\n",
              "     9       ...  109524  109525  109526  109527  109528  109529  109530  \\\n",
              "0       957  ...     970     984     970     968     972     986     977   \n",
              "1       960  ...     973     981     969     971     971     986     977   \n",
              "2       959  ...     975     980     975     969     970     985     977   \n",
              "3       961  ...     973     978     979     966     971     982     978   \n",
              "4       960  ...     972     980     981     965     970     980     978   \n",
              "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "251     962  ...     991     975     966     975     987     985     970   \n",
              "252     960  ...     990     975     968     974     988     983     969   \n",
              "253     961  ...     989     975     970     971     987     981     970   \n",
              "254     961  ...     984     972     971     970     986     979     970   \n",
              "255     964  ...     983     971     970     973     986     979     971   \n",
              "\n",
              "     109531  109532  109533  \n",
              "0       969     977     987  \n",
              "1       970     979     986  \n",
              "2       970     983     987  \n",
              "3       971     982     988  \n",
              "4       971     981     988  \n",
              "..      ...     ...     ...  \n",
              "251     978     987     967  \n",
              "252     977     985     966  \n",
              "253     979     987     962  \n",
              "254     976     986     961  \n",
              "255     978     988     962  \n",
              "\n",
              "[256 rows x 109534 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(X[:])\n",
        "df = df.T\n",
        "df.to_csv(\"ECG_signals.csv\", index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUKeCRNHqVTs",
        "outputId": "f6a4d3e8-8d8b-4281-a5cb-fd6ce7b07c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "data1 = []\n",
        "\n",
        "for i in range(10):\n",
        "    img_folder = f'./ECG/Difference/images{i+1}'\n",
        "    img_data_array=[]\n",
        "    for file in os.listdir(os.path.join(img_folder)):\n",
        "\n",
        "        image_path= os.path.join(img_folder, file)\n",
        "        image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
        "        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
        "        image=np.array(image)\n",
        "        image = image.astype('float16')\n",
        "        image /= 255\n",
        "        img_data_array.append(image)\n",
        "    data1.append(img_data_array)\n",
        "    print(i)\n",
        "print(len(data1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRaiqXHNqVTt",
        "outputId": "8babdaa1-e3c2-4136-c139-b9f1eccc1442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87623 87623\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trains1 = []\n",
        "X_tests1 = []\n",
        "y_trains1 = []\n",
        "y_tests1 = []\n",
        "label = []\n",
        "for i in range(10):\n",
        "    temp = [i for j in range(y.count(i))]\n",
        "    label.append(temp)\n",
        "temp = list(data1[0])\n",
        "X_trains1, X_tests1, y_trains1, y_tests1 = train_test_split(temp, label[0], test_size=0.2, random_state=1)\n",
        "\n",
        "for i in range(1, 10):\n",
        "    temp = list(data1[i])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(temp, label[i], test_size=0.2, random_state=1)\n",
        "\n",
        "    # Merge inputs and targets\n",
        "    X_trains1 = np.concatenate((X_trains1, X_train), axis=0)\n",
        "    X_tests1 = np.concatenate((X_tests1, X_test), axis=0)\n",
        "    y_trains1 = np.concatenate((y_trains1, y_train), axis=0)\n",
        "    y_tests1 = np.concatenate((y_tests1, y_test), axis=0)\n",
        "\n",
        "print(len(X_trains1), len(y_trains1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8chgmKDqVTt",
        "outputId": "2f3f9074-5db9-43e9-a49e-345ff005f989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 643,658\n",
            "Trainable params: 643,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Finish!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Model configuration\n",
        "batch_size = 32\n",
        "input_shape = (32, 32, 3)\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 10\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 10\n",
        "\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs1 = np.concatenate((X_trains1, X_tests1), axis=0)\n",
        "targets1 = np.concatenate((y_trains1, y_tests1), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Define the model architecture\n",
        "model1 = Sequential()\n",
        "model1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(256, activation='relu'))\n",
        "model1.add(Dense(128, activation='relu'))\n",
        "model1.add(Dense(no_classes, activation='softmax'))\n",
        "print(model1.summary())\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(loss=loss_function,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "print('Finish!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5ZZsT97qVTt",
        "outputId": "3ddb6502-aba3-4138-b5f6-c2ae0ba15280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 53s 17ms/step - loss: 0.1842 - accuracy: 0.9506\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0759 - accuracy: 0.9794\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 81s 26ms/step - loss: 0.0577 - accuracy: 0.9837\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0486 - accuracy: 0.9860\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0420 - accuracy: 0.9880\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0373 - accuracy: 0.9894\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0325 - accuracy: 0.9901\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0307 - accuracy: 0.9910\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0282 - accuracy: 0.9916\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0259 - accuracy: 0.9921\n",
            "Score for fold 1: loss of 0.05199152231216431; accuracy of 98.62150549888611%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0280 - accuracy: 0.9920\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 82s 27ms/step - loss: 0.0241 - accuracy: 0.9928\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0224 - accuracy: 0.9933\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 71s 23ms/step - loss: 0.0213 - accuracy: 0.9934\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0187 - accuracy: 0.9941\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0189 - accuracy: 0.9941\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0186 - accuracy: 0.9942\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0165 - accuracy: 0.9951\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0171 - accuracy: 0.9948\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0162 - accuracy: 0.9951\n",
            "Score for fold 2: loss of 0.031159762293100357; accuracy of 99.2422878742218%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0189 - accuracy: 0.9944\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0153 - accuracy: 0.9954\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0157 - accuracy: 0.9950\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0151 - accuracy: 0.9953\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0146 - accuracy: 0.9954\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0136 - accuracy: 0.9960\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0146 - accuracy: 0.9957\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0147 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 69s 23ms/step - loss: 0.0134 - accuracy: 0.9960\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0131 - accuracy: 0.9960\n",
            "Score for fold 3: loss of 0.020790966227650642; accuracy of 99.40661191940308%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 62s 20ms/step - loss: 0.0157 - accuracy: 0.9953\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0138 - accuracy: 0.9961\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 66s 22ms/step - loss: 0.0138 - accuracy: 0.9958\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0131 - accuracy: 0.9960\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0118 - accuracy: 0.9961\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0116 - accuracy: 0.9962\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0121 - accuracy: 0.9964\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0121 - accuracy: 0.9962\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0098 - accuracy: 0.9971\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0118 - accuracy: 0.9964\n",
            "Score for fold 4: loss of 0.018943728879094124; accuracy of 99.37922358512878%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0126 - accuracy: 0.9963\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0129 - accuracy: 0.9965\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0120 - accuracy: 0.9964\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0114 - accuracy: 0.9968\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0098 - accuracy: 0.9970\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0119 - accuracy: 0.9964\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0100 - accuracy: 0.9970\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 75s 24ms/step - loss: 0.0104 - accuracy: 0.9968\n",
            "Score for fold 5: loss of 0.028834830969572067; accuracy of 99.3882954120636%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 74s 24ms/step - loss: 0.0133 - accuracy: 0.9964\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0109 - accuracy: 0.9969\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0110 - accuracy: 0.9967\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 64s 21ms/step - loss: 0.0112 - accuracy: 0.9968\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0109 - accuracy: 0.9971\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 70s 23ms/step - loss: 0.0137 - accuracy: 0.9965\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 71s 23ms/step - loss: 0.0095 - accuracy: 0.9973\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0099 - accuracy: 0.9971\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 67s 22ms/step - loss: 0.0101 - accuracy: 0.9971\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0115 - accuracy: 0.9969\n",
            "Score for fold 6: loss of 0.01931675896048546; accuracy of 99.47959184646606%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0112 - accuracy: 0.9969\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0109 - accuracy: 0.9971\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0084 - accuracy: 0.9974\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 68s 22ms/step - loss: 0.0087 - accuracy: 0.9975\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0115 - accuracy: 0.9965\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0105 - accuracy: 0.9970\n",
            "Score for fold 7: loss of 0.010710549540817738; accuracy of 99.72610473632812%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 79s 26ms/step - loss: 0.0115 - accuracy: 0.9969\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 65s 21ms/step - loss: 0.0094 - accuracy: 0.9973\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 77s 25ms/step - loss: 0.0111 - accuracy: 0.9970\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0105 - accuracy: 0.9972\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 72s 23ms/step - loss: 0.0105 - accuracy: 0.9973\n",
            "Score for fold 8: loss of 0.00841793604195118; accuracy of 99.77174997329712%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0100 - accuracy: 0.9971\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 66s 22ms/step - loss: 0.0117 - accuracy: 0.9968\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 80s 26ms/step - loss: 0.0126 - accuracy: 0.9970\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 66s 21ms/step - loss: 0.0116 - accuracy: 0.9972\n",
            "Score for fold 9: loss of 0.010919823311269283; accuracy of 99.75349307060242%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 78s 25ms/step - loss: 0.0096 - accuracy: 0.9973\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 73s 24ms/step - loss: 0.0112 - accuracy: 0.9971\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 70s 23ms/step - loss: 0.0107 - accuracy: 0.9972\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 76s 25ms/step - loss: 0.0110 - accuracy: 0.9973\n",
            "Score for fold 10: loss of 0.021101614460349083; accuracy of 99.60741400718689%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs1, targets1):\n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    # Fit data to model\n",
        "    history1 = model1.fit(inputs1[train], targets1[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity,\n",
        "              callbacks=[callback])\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores1 = model1.evaluate(inputs1[test], targets1[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model1.metrics_names[0]} of {scores1[0]}; {model1.metrics_names[1]} of {scores1[1]*100}%')\n",
        "    acc_per_fold.append(scores1[1] * 100)\n",
        "    loss_per_fold.append(scores1[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ec_chT9qVTt",
        "outputId": "9e600eab-fcf6-47dc-9f58-7ee45c9cde72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "data = []\n",
        "\n",
        "for i in range(10):\n",
        "    img_folder = f'./ECG/Summation/images{i+1}'\n",
        "    img_data_array=[]\n",
        "    for file in os.listdir(os.path.join(img_folder)):\n",
        "        image_path= os.path.join(img_folder, file)\n",
        "        image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
        "        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n",
        "        image=np.array(image)\n",
        "        image = image.astype('float16')\n",
        "        image /= 255\n",
        "        img_data_array.append(image)\n",
        "    data.append(img_data_array)\n",
        "    print(i)\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD4BY4PvqVTt",
        "outputId": "f50538f3-6b75-4150-9ff8-9cb604b97c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87623 87623\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_trains = []\n",
        "X_tests = []\n",
        "y_trains = []\n",
        "y_tests = []\n",
        "label = []\n",
        "for i in range(10):\n",
        "    temp = [i for j in range(y.count(i))]\n",
        "    label.append(temp)\n",
        "temp = list(data[0])\n",
        "X_trains, X_tests, y_trains, y_tests = train_test_split(temp, label[0], test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "for i in range(1, 10):\n",
        "    temp = list(data[i])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(temp, label[i], test_size=0.2, random_state=1)\n",
        "\n",
        "    # Merge inputs and targets\n",
        "    X_trains = np.concatenate((X_trains, X_train), axis=0)\n",
        "    X_tests = np.concatenate((X_tests, X_test), axis=0)\n",
        "    y_trains = np.concatenate((y_trains, y_train), axis=0)\n",
        "    y_tests = np.concatenate((y_tests, y_test), axis=0)\n",
        "\n",
        "print(len(X_trains), len(y_trains))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1_ngkc5qVTu",
        "outputId": "e2b1802b-d4a5-4081-e972-d4153e6d4eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 15, 15, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  (None, 6, 6, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 643,658\n",
            "Trainable params: 643,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Finish!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Model configuration\n",
        "batch_size = 32\n",
        "input_shape = (32, 32, 3)\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 10\n",
        "no_epochs = 10\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 10\n",
        "\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((X_trains, X_tests), axis=0)\n",
        "targets = np.concatenate((y_trains, y_tests), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(no_classes, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=loss_function,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "print('Finish!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTOhNMQEqVTu",
        "outputId": "a6bee4b3-db53-405e-8e72-d56fb33f93db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 78s 25ms/step - loss: 0.2068 - accuracy: 0.9459\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 143s 46ms/step - loss: 0.0873 - accuracy: 0.9762\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0678 - accuracy: 0.9810\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0562 - accuracy: 0.9838\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0484 - accuracy: 0.9862\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0426 - accuracy: 0.9876\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0384 - accuracy: 0.9884\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 102s 33ms/step - loss: 0.0340 - accuracy: 0.9897\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0304 - accuracy: 0.9909\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0294 - accuracy: 0.9911\n",
            "Score for fold 1: loss of 0.07727085053920746; accuracy of 98.33850860595703%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 98s 32ms/step - loss: 0.0318 - accuracy: 0.9907\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0272 - accuracy: 0.9917\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 105s 34ms/step - loss: 0.0253 - accuracy: 0.9923\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0245 - accuracy: 0.9924\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 103s 34ms/step - loss: 0.0212 - accuracy: 0.9938\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 105s 34ms/step - loss: 0.0219 - accuracy: 0.9932\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0201 - accuracy: 0.9936\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 104s 34ms/step - loss: 0.0193 - accuracy: 0.9939\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 120s 39ms/step - loss: 0.0178 - accuracy: 0.9945\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0186 - accuracy: 0.9942\n",
            "Score for fold 2: loss of 0.04149759188294411; accuracy of 99.05057549476624%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0207 - accuracy: 0.9940\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 103s 34ms/step - loss: 0.0183 - accuracy: 0.9945\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 102s 33ms/step - loss: 0.0169 - accuracy: 0.9949\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0169 - accuracy: 0.9949\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0155 - accuracy: 0.9953\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0161 - accuracy: 0.9953\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0154 - accuracy: 0.9953\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0151 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 101s 33ms/step - loss: 0.0140 - accuracy: 0.9957\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 107s 35ms/step - loss: 0.0151 - accuracy: 0.9956\n",
            "Score for fold 3: loss of 0.037436820566654205; accuracy of 99.08708930015564%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0164 - accuracy: 0.9950\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 106s 34ms/step - loss: 0.0135 - accuracy: 0.9961\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0147 - accuracy: 0.9954\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 109s 35ms/step - loss: 0.0127 - accuracy: 0.9961\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0135 - accuracy: 0.9960\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0133 - accuracy: 0.9961\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0133 - accuracy: 0.9962\n",
            "Score for fold 4: loss of 0.021438004449009895; accuracy of 99.3883490562439%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0140 - accuracy: 0.9960\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0134 - accuracy: 0.9960\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 108s 35ms/step - loss: 0.0122 - accuracy: 0.9962\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0135 - accuracy: 0.9962\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 125s 40ms/step - loss: 0.0133 - accuracy: 0.9961\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0123 - accuracy: 0.9962\n",
            "Score for fold 5: loss of 0.023886283859610558; accuracy of 99.3609070777893%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0132 - accuracy: 0.9960\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 111s 36ms/step - loss: 0.0130 - accuracy: 0.9962\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 109s 35ms/step - loss: 0.0115 - accuracy: 0.9965\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 118s 38ms/step - loss: 0.0131 - accuracy: 0.9963\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0115 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 124s 40ms/step - loss: 0.0108 - accuracy: 0.9966\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 141s 46ms/step - loss: 0.0135 - accuracy: 0.9962\n",
            "Epoch 8/10\n",
            "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0100 - accuracy: 0.9969\n",
            "Epoch 9/10\n",
            "3081/3081 [==============================] - 113s 37ms/step - loss: 0.0120 - accuracy: 0.9963\n",
            "Epoch 10/10\n",
            "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0108 - accuracy: 0.9967\n",
            "Score for fold 6: loss of 0.01732526533305645; accuracy of 99.55263137817383%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0133 - accuracy: 0.9966\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0116 - accuracy: 0.9966\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0109 - accuracy: 0.9970\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0115 - accuracy: 0.9966\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0112 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0122 - accuracy: 0.9966\n",
            "Score for fold 7: loss of 0.011116073466837406; accuracy of 99.67132210731506%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 119s 39ms/step - loss: 0.0097 - accuracy: 0.9973\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 110s 36ms/step - loss: 0.0123 - accuracy: 0.9966\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 112s 36ms/step - loss: 0.0116 - accuracy: 0.9968\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0114 - accuracy: 0.9967\n",
            "Score for fold 8: loss of 0.014107241295278072; accuracy of 99.65306520462036%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 118s 38ms/step - loss: 0.0120 - accuracy: 0.9967\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 123s 40ms/step - loss: 0.0114 - accuracy: 0.9969\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0111 - accuracy: 0.9969\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 120s 39ms/step - loss: 0.0102 - accuracy: 0.9972\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0102 - accuracy: 0.9973\n",
            "Epoch 6/10\n",
            "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0103 - accuracy: 0.9971\n",
            "Epoch 7/10\n",
            "3081/3081 [==============================] - 115s 37ms/step - loss: 0.0105 - accuracy: 0.9970\n",
            "Score for fold 9: loss of 0.016758309677243233; accuracy of 99.52524304389954%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/10\n",
            "3081/3081 [==============================] - 114s 37ms/step - loss: 0.0142 - accuracy: 0.9965\n",
            "Epoch 2/10\n",
            "3081/3081 [==============================] - 116s 38ms/step - loss: 0.0108 - accuracy: 0.9970\n",
            "Epoch 3/10\n",
            "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0115 - accuracy: 0.9969\n",
            "Epoch 4/10\n",
            "3081/3081 [==============================] - 121s 39ms/step - loss: 0.0111 - accuracy: 0.9971\n",
            "Epoch 5/10\n",
            "3081/3081 [==============================] - 117s 38ms/step - loss: 0.0109 - accuracy: 0.9970\n",
            "Score for fold 10: loss of 0.007950237020850182; accuracy of 99.7900128364563%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "    # Generate a print\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "    # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=no_epochs,\n",
        "              verbose=verbosity,\n",
        "              callbacks=[callback])\n",
        "\n",
        "    # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q038EtA4qVTu",
        "outputId": "3cd1ed05-6f1e-46d7-cb4c-d11f32d6ef73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "685/685 [==============================] - 5s 8ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9523    1.0000    0.9756     15005\n",
            "           1     1.0000    0.9882    0.9941      1615\n",
            "           2     1.0000    0.8415    0.9139      1451\n",
            "           3     1.0000    0.6961    0.8208       510\n",
            "           4     1.0000    0.8962    0.9453      1426\n",
            "           5     1.0000    0.9907    0.9954      1405\n",
            "           6     1.0000    0.8122    0.8964       197\n",
            "           7     1.0000    0.2174    0.3571       161\n",
            "           8     1.0000    1.0000    1.0000        95\n",
            "           9     1.0000    0.5000    0.6667        46\n",
            "\n",
            "    accuracy                         0.9657     21911\n",
            "   macro avg     0.9952    0.7942    0.8565     21911\n",
            "weighted avg     0.9674    0.9657    0.9628     21911\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "preds = model.predict(X_tests)\n",
        "preds = np.argmax(preds.astype('int'), axis=1)\n",
        "y_tests = y_tests.flatten()\n",
        "print(classification_report(y_tests, preds, digits=4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}